{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Hand Gesture Recognition (Checkpoint 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This checkpoint is at the end of Objective 3.\n",
    "\n",
    "At this point, your code should be able to average the backgrounds, segmented the current frame to separate the object from the region of interest, and display it for viewing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Header: Importing libraries and creating global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# Hold the background frame for background subtraction.\n",
    "background = None\n",
    "# Hold the hand's data so all its details are in one place.\n",
    "hand = None\n",
    "# Variables to count how many frames have passed and to set the size of the window.\n",
    "frames_elapsed = 0\n",
    "FRAME_HEIGHT = 200\n",
    "FRAME_WIDTH = 300\n",
    "# Humans come in a ton of beautiful shades and colors.\n",
    "# Try editing these if your program has trouble recognizing your skin tone.\n",
    "CALIBRATION_TIME = 30\n",
    "BG_WEIGHT = 0.5\n",
    "OBJ_THRESHOLD = 18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HandData: A class to hold all the hand's details and flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HandData:\n",
    "    top = (0,0)\n",
    "    bottom = (0,0)\n",
    "    left = (0,0)\n",
    "    right = (0,0)\n",
    "    centerX = 0\n",
    "    prevCenterX = 0\n",
    "    isInFrame = False\n",
    "    isWaving = False\n",
    "    fingers = None\n",
    "    \n",
    "    def __init__(self, top, bottom, left, right, centerX):\n",
    "        self.top = top\n",
    "        self.bottom = bottom\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.centerX = centerX\n",
    "        self.prevCenterX = 0\n",
    "        isInFrame = False\n",
    "        isWaving = False\n",
    "        \n",
    "    def update(self, top, bottom, left, right):\n",
    "        self.top = top\n",
    "        self.bottom = bottom\n",
    "        self.left = left\n",
    "        self.right = right"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### write_on_image(): Write info related to the hand gesture and outline the region of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we take the current frame, the number of frames elapsed, and how many fingers we've detected\n",
    "# so we can print on the screen which gesture is happening (or if the camera is calibrating).\n",
    "def write_on_image(frame):\n",
    "    text = \"Searching...\"\n",
    "\n",
    "    if frames_elapsed < CALIBRATION_TIME:\n",
    "        text = \"Calibrating...\"\n",
    "    elif hand == None or hand.isInFrame == False:\n",
    "        text = \"No hand detected\"\n",
    "    else:\n",
    "        if hand.isWaving:\n",
    "            text = \"Waving\"\n",
    "        elif hand.fingers == 0:\n",
    "            text = \"Rock\"\n",
    "        elif hand.fingers == 1:\n",
    "            text = \"Pointing\"\n",
    "        elif hand.fingers == 2:\n",
    "            text = \"Scissors\"\n",
    "    \n",
    "    cv2.putText(frame, text, (10,20), cv2.FONT_HERSHEY_COMPLEX, 0.4,( 0 , 0 , 0 ),2,cv2.LINE_AA)\n",
    "    cv2.putText(frame, text, (10,20), cv2.FONT_HERSHEY_COMPLEX, 0.4,(255,255,255),1,cv2.LINE_AA)\n",
    "\n",
    "    # Highlight the region of interest.\n",
    "    cv2.rectangle(frame, (region_left, region_top), (region_right, region_bottom), (255,255,255), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get_region(): Separate the region of interest and preps it for edge detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_region(frame):\n",
    "    # Separate the region of interest from the rest of the frame.\n",
    "    region = frame[region_top:region_bottom, region_left:region_right]\n",
    "    # Make it grayscale so we can detect the edges more easily.\n",
    "    region = cv2.cvtColor(region, cv2.COLOR_BGR2GRAY)\n",
    "    # Use a Gaussian blur to prevent frame noise from being labeled as an edge.\n",
    "    region = cv2.GaussianBlur(region, (5,5), 0)\n",
    "\n",
    "    return region"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get_average(): Create a weighted average of the background for image differencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average(region):\n",
    "    # We have to use the global keyword because we want to edit the global variable.\n",
    "    global background\n",
    "    # If we haven't captured the background yet, make the current region the background.\n",
    "    if background is None:\n",
    "        background = region.copy().astype(\"float\")\n",
    "        return\n",
    "    # Otherwise, add this captured frame to the average of the backgrounds.\n",
    "    cv2.accumulateWeighted(region, background, BG_WEIGHT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### segment(): Use image differencing to separate the hand from the background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we use differencing to separate the background from the object of interest.\n",
    "def segment(region):\n",
    "    global hand\n",
    "    # Find the absolute difference between the background and the current frame.\n",
    "    diff = cv2.absdiff(background.astype(np.uint8), region)\n",
    "\n",
    "    # Threshold that region with a strict 0 or 1 ruling so only the foreground remains.\n",
    "    thresholded_region = cv2.threshold(diff, OBJ_THRESHOLD, 255, cv2.THRESH_BINARY)[1]\n",
    "\n",
    "    # Get the contours of the region, which will return an outline of the hand.\n",
    "    (_, contours, _) = cv2.findContours(thresholded_region.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # If we didn't get anything, there's no hand.\n",
    "    if len(contours) == 0:\n",
    "        if hand is not None:\n",
    "            hand.isInFrame = False\n",
    "        return\n",
    "    # Otherwise return a tuple of the filled hand (thresholded_region), along with the outline (segmented_region).\n",
    "    else:\n",
    "        if hand is not None:\n",
    "            hand.isInFrame = True\n",
    "        segmented_region = max(contours, key = cv2.contourArea)\n",
    "        return (thresholded_region, segmented_region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main function: Get input from camera and call functions to understand it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenCV: out device of bound (0-0): 1\n",
      "OpenCV: camera failed to properly initialize!\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.9.0) /Users/xperience/GHA-OpenCV-Python2/_work/opencv-python/opencv-python/opencv/modules/imgproc/src/resize.cpp:4152: error: (-215:Assertion failed) !ssize.empty() in function 'resize'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# Store the frame from the video capture and resize it to the window size.\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     ret, frame \u001b[38;5;241m=\u001b[39m capture\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m---> 14\u001b[0m     frame \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mFRAME_WIDTH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mFRAME_HEIGHT\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m# Flip the frame over the vertical axis so that it works like a mirror, which is more intuitive to the user.\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     frame \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mflip(frame, \u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.9.0) /Users/xperience/GHA-OpenCV-Python2/_work/opencv-python/opencv-python/opencv/modules/imgproc/src/resize.cpp:4152: error: (-215:Assertion failed) !ssize.empty() in function 'resize'\n"
     ]
    }
   ],
   "source": [
    "# Our region of interest will be the top right part of the frame.\n",
    "region_top = 0\n",
    "region_bottom = int(2 * FRAME_HEIGHT / 3)\n",
    "region_left = int(FRAME_WIDTH / 2)\n",
    "region_right = FRAME_WIDTH\n",
    "\n",
    "frames_elapsed = 0\n",
    "\n",
    "capture = cv2.VideoCapture(0)\n",
    "\n",
    "while (True):\n",
    "    # Store the frame from the video capture and resize it to the window size.\n",
    "    ret, frame = capture.read()\n",
    "    frame = cv2.resize(frame, (FRAME_WIDTH, FRAME_HEIGHT))\n",
    "    # Flip the frame over the vertical axis so that it works like a mirror, which is more intuitive to the user.\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    \n",
    "    # Separate the region of interest and prep it for edge detection.\n",
    "    region = get_region(frame)\n",
    "    if frames_elapsed < CALIBRATION_TIME:\n",
    "        get_average(region)\n",
    "    else:\n",
    "        region_pair = segment(region)\n",
    "        if region_pair is not None:\n",
    "            # If we have the regions segmented successfully, show them in another window for the user.\n",
    "            (thresholded_region, segmented_region) = region_pair\n",
    "            cv2.drawContours(region, [segmented_region], -1, (255, 255, 255))\n",
    "            cv2.imshow(\"Segmented Image\", region)\n",
    "    \n",
    "    # Write the action the hand is doing on the screen, and draw the region of interest.\n",
    "    write_on_image(frame)\n",
    "    # Show the previously captured frame.\n",
    "    cv2.imshow(\"Camera Input\", frame)\n",
    "    frames_elapsed += 1\n",
    "    # Check if user wants to exit.\n",
    "    if (cv2.waitKey(1) & 0xFF == ord('x')):\n",
    "        break\n",
    "\n",
    "# When we exit the loop, we have to stop the capture too.\n",
    "capture.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
